{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feattures.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl-KHPBKQcZl"
      },
      "source": [
        "### Having_ip_address"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzlUVRqYQbu3"
      },
      "source": [
        "def having_ip_address(url):\n",
        "  import re\n",
        "  x = re.search('^(http|https)://\\d+\\.\\d+\\.\\d+\\.\\d+\\.*', url)\n",
        "  if x:\n",
        "    return 1\n",
        "  else:\n",
        "    return -1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKtLD4xYQ8N2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95031afa-665b-4234-8e83-6fade4872d7e"
      },
      "source": [
        "having_ip_address('https://128.36.54.192/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFiiEEkHT-cW"
      },
      "source": [
        "### URL Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLWmOZPeUDym"
      },
      "source": [
        "def URL_Length(url):\n",
        "  if len(url) < 54:\n",
        "    return -1\n",
        "  elif len(url) >= 54 and len(url) <=75:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgZRmWZYpxVV",
        "outputId": "109f2562-7d93-4264-893f-9a8944f9923b"
      },
      "source": [
        "URL_Length('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1xmDOAKWVRN"
      },
      "source": [
        "### Having @ Symbol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5G-TRuDUSrY"
      },
      "source": [
        "def haveAtSign(url):\n",
        "  if \"@\" in url:\n",
        "    at = 1    \n",
        "  else:\n",
        "    at = -1    \n",
        "  return at"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gghA3QASqFYD",
        "outputId": "403cfadd-b908-4c5e-9401-997c822e7b8e"
      },
      "source": [
        "haveAtSign('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkegNyKBWZzW"
      },
      "source": [
        "### Prefix_Suffix  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VegEMYw-WOqO"
      },
      "source": [
        "def prefixSuffix(url):\n",
        "    from urllib.parse import urlparse\n",
        "    if '-' in urlparse(url).netloc:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqwOHhZUXkaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11bb6fac-0409-4e2a-ce26-2f238b466763"
      },
      "source": [
        "prefixSuffix('https://dfhdfhdfgs.tokyo/ja-jp/account/login')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T8WHUx1KW4d"
      },
      "source": [
        "### Having sub domain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdRTQYEqKdmT"
      },
      "source": [
        "def sub_domain_count(url):\n",
        "  !pip install tld\n",
        "  from urllib.parse import urlparse\n",
        "  from tld import get_tld\n",
        "  domain = urlparse(url).netloc\n",
        "  domain = domain.split('.')\n",
        "  top_domain = get_tld(url)\n",
        "  top_domain = top_domain.split('.')\n",
        "  sub_domain = set(domain) - set(top_domain)\n",
        "  count = len(sub_domain)\n",
        "  if(count == 1):\n",
        "    return -1\n",
        "  if(count == 2):\n",
        "    return 0\n",
        "  else:\n",
        "    return 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNK0jN-1KijH",
        "outputId": "2cae26a9-27a4-4e89-81fa-542ea48c1a28"
      },
      "source": [
        "sub_domain_count('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tld\n",
            "  Downloading tld-0.12.6-py37-none-any.whl (412 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 184 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 194 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 204 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 215 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 225 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 235 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 245 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 256 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 266 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 276 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 286 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 296 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 307 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 317 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 327 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 337 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 348 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 358 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 368 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 378 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 389 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 399 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 409 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 412 kB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: tld\n",
            "Successfully installed tld-0.12.6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NAAy8DRHbl5"
      },
      "source": [
        "###SSL final state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MF8WvH2HiSn"
      },
      "source": [
        "def sslVerify(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from urllib.request import Request, urlopen, ssl, socket\n",
        "  import json\n",
        "  from datetime import datetime\n",
        "  split_url = urlsplit(url)\n",
        "  #some site without http/https in the path\n",
        "  port = '443'\n",
        "\n",
        "  hostname = split_url.netloc\n",
        "  context = ssl.create_default_context()\n",
        "  try:\n",
        "    with socket.create_connection((hostname, port)) as sock:\n",
        "      with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n",
        "          data = json.dumps(ssock.getpeercert())\n",
        "          res = json.loads(data)\n",
        "    notBefore = datetime.strptime(res[\"notBefore\"],'%b  %d %H:%M:%S %Y %Z').date()\n",
        "    notAfter = datetime.strptime(res[\"notAfter\"],'%b  %d %H:%M:%S %Y %Z').date()\n",
        "    # print(notBefore, notAfter)\n",
        "    if(ssl.SSLCertVerificationError(ssock) == True):\n",
        "      return 1\n",
        "    elif (notAfter.year-notBefore.year)+(notAfter.month-notBefore.month)*0.1 >= 1:\n",
        "      return -1\n",
        "    else:\n",
        "      return 0\n",
        "  except:\n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBOeXVpvH2lL",
        "outputId": "3928ba15-455c-4236-c35b-36cf45dfe007"
      },
      "source": [
        "sslVerify(\"http://postdebanks.com/DIE/POST/diepost/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0m-feIvD6YF"
      },
      "source": [
        "### Port"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPUhwNcED8x6"
      },
      "source": [
        "def port(domain):\n",
        "  import requests\n",
        "  import json\n",
        "  response = requests.get(\"https://api.viewdns.info/portscan/?host=\"+domain+\"&apikey=1bf03196763a201bcc66a59bf88ed8ddf7a9432f&output=json\")\n",
        "  myjson = response.json()\n",
        "  # print(myjson)\n",
        "  pref_stat = {21:'closed', 22:'closed', 23:'closed', 80:'open', 443:'open', 445 : 'closed', 1433:'closed', 1521:'closed', 3306 : 'closed', 3389:'closed' }\n",
        "  flag = -1\n",
        "  for i in range(len(myjson['response']['port'])):\n",
        "    if int(myjson['response']['port'][i]['number']) in pref_stat:\n",
        "      if(myjson['response']['port'][0]['status'] != pref_stat[int(myjson['response']['port'][0]['number'])]):\n",
        "        flag = 1\n",
        "  return flag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcvJ8T_MEEws"
      },
      "source": [
        "# port('postdebanks.com')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN1VuL2HsN5r"
      },
      "source": [
        "### Request_URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53pqH-W49_ss"
      },
      "source": [
        "def request_url(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        print('Yes')\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  else:\n",
        "        print('No state')\n",
        "  \n",
        "  Media = {'internals':[], 'externals':[], 'null':[]}\n",
        "\n",
        "  def external_media(Media):\n",
        "    total = len(Media['internals']) + len(Media['externals'])\n",
        "    externals = len(Media['externals'])\n",
        "    try:\n",
        "        percentile = externals / float(total) * 100\n",
        "    except:\n",
        "        return 0\n",
        "    return percentile\n",
        "  \n",
        "  def findMedia(Media,domain, hostname ):\n",
        "    Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "    soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
        "    for img in soup.find_all('img', src=True):\n",
        "          dots = [x.start(0) for x in re.finditer('\\.', img['src'])]\n",
        "          if hostname in img['src'] or domain in img['src'] or len(dots) == 1 or not img['src'].startswith('http'):\n",
        "              if not img['src'].startswith('http'):\n",
        "                  if not img['src'].startswith('/'):\n",
        "                      Media['internals'].append(hostname+'/'+img['src']) \n",
        "                  elif img['src'] in Null_format:\n",
        "                      Media['null'].append(img['src'])  \n",
        "                  else:\n",
        "                      Media['internals'].append(hostname+img['src'])   \n",
        "          else:\n",
        "              Media['externals'].append(img['src'])\n",
        "    for audio in soup.find_all('audio', src=True):\n",
        "      dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
        "      if hostname in audio['src'] or domain in audio['src'] or len(dots) == 1 or not audio['src'].startswith('http'):\n",
        "          if not audio['src'].startswith('http'):\n",
        "              if not audio['src'].startswith('/'):\n",
        "                  Media['internals'].append(hostname+'/'+audio['src']) \n",
        "              elif audio['src'] in Null_format:\n",
        "                  Media['null'].append(audio['src'])  \n",
        "              else:\n",
        "                  Media['internals'].append(hostname+audio['src'])   \n",
        "      else:\n",
        "          Media['externals'].append(audio['src'])\n",
        "          \n",
        "    for embed in soup.find_all('embed', src=True):\n",
        "      dots = [x.start(0) for x in re.finditer('\\.', embed['src'])]\n",
        "      if hostname in embed['src'] or domain in embed['src'] or len(dots) == 1 or not embed['src'].startswith('http'):\n",
        "          if not embed['src'].startswith('http'):\n",
        "              if not embed['src'].startswith('/'):\n",
        "                  Media['internals'].append(hostname+'/'+embed['src']) \n",
        "              elif embed['src'] in Null_format:\n",
        "                  Media['null'].append(embed['src'])  \n",
        "              else:\n",
        "                  Media['internals'].append(hostname+embed['src'])   \n",
        "      else:\n",
        "          Media['externals'].append(embed['src'])\n",
        "        \n",
        "    for i_frame in soup.find_all('iframe', src=True):\n",
        "      dots = [x.start(0) for x in re.finditer('\\.', i_frame['src'])]\n",
        "      if hostname in i_frame['src'] or domain in i_frame['src'] or len(dots) == 1 or not i_frame['src'].startswith('http'):\n",
        "          if not i_frame['src'].startswith('http'):\n",
        "              if not i_frame['src'].startswith('/'):\n",
        "                  Media['internals'].append(hostname+'/'+i_frame['src']) \n",
        "              elif i_frame['src'] in Null_format:\n",
        "                  Media['null'].append(i_frame['src'])  \n",
        "              else:\n",
        "                  Media['internals'].append(hostname+i_frame['src'])   \n",
        "      else: \n",
        "          Media['externals'].append(i_frame['src'])\n",
        "  \n",
        "  findMedia(Media, domain, hostname)\n",
        "  if external_media(Media) < 22:\n",
        "    return -1\n",
        "  elif external_media(Media) >= 22 and external_media(Media) < 61:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D03EVESN_Q18"
      },
      "source": [
        "# request_url('http://www.budgetbots.com/server.php/Server%20update/index.php?email=USER@DOMAIN.com')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kZqNCVNbI8n"
      },
      "source": [
        "### Safe anchors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXhH39Y2slwN"
      },
      "source": [
        "def url_anchor(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "    \n",
        "  Anchor = {'safe':[], 'unsafe':[], 'null':[]}\n",
        "\n",
        "  def anchor(Anchor, domain, hostname):\n",
        "      soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
        "      for href in soup.find_all('a', href=True):\n",
        "        dots = [x.start(0) for x in re.finditer('\\.', href['href'])]\n",
        "        if hostname in href['href'] or domain in href['href'] or len(dots) == 1 or not href['href'].startswith('http'):\n",
        "            if \"#\" in href['href'] or \"javascript\" in href['href'].lower() or \"mailto\" in href['href'].lower():\n",
        "                 Anchor['unsafe'].append(href['href'])\n",
        "        else:\n",
        "            Anchor['safe'].append(href['href'])\n",
        "\n",
        "  anchor(Anchor, domain, url)\n",
        "\n",
        "  def safe_anchor(Anchor):\n",
        "      total = len(Anchor['safe']) +  len(Anchor['unsafe'])\n",
        "      unsafe = len(Anchor['unsafe'])\n",
        "      try:\n",
        "        percentile = unsafe / float(total) * 100\n",
        "      except:\n",
        "        return 0\n",
        "      return percentile\n",
        "  #print(safe_anchor(Anchor))   \n",
        "  if safe_anchor(Anchor) < 31:\n",
        "       return -1\n",
        "  elif safe_anchor(Anchor) >= 31 and safe_anchor(Anchor) <= 67:\n",
        "       return 0\n",
        "  else:\n",
        "       return 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3T8ZDedxchB",
        "outputId": "c14f39e1-9cfe-4057-a114-ed0374892cef"
      },
      "source": [
        "url_anchor('https://www.xiaoji.com/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tldextract\n",
            "  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 30 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 51 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 81 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 87 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.23.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.10)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract) (3.4.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from requests-file>=1.4->tldextract) (1.15.0)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-1.5.1 tldextract-3.1.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2ChpOldzG4c"
      },
      "source": [
        "### Links in Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyEfGEeUzePr"
      },
      "source": [
        "def links_tag(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  \n",
        "  Link = {'internals':[], 'externals':[], 'null':[]}\n",
        "\n",
        "  def find_links(Link, domain, hostname):\n",
        "    soup = BeautifulSoup(content, 'html.parser', from_encoding='iso-8859-1')\n",
        "    for link in soup.findAll('link', href=True):\n",
        "        dots = [x.start(0) for x in re.finditer('\\.', link['href'])]\n",
        "        if hostname in link['href'] or domain in link['href'] or len(dots) == 1 or not link['href'].startswith('http'):\n",
        "            if not link['href'].startswith('http'):\n",
        "                if not link['href'].startswith('/'):\n",
        "                    Link['internals'].append(hostname+'/'+link['href']) \n",
        "                elif link['href'] in Null_format:\n",
        "                    Link['null'].append(link['href'])  \n",
        "                else:\n",
        "                    Link['internals'].append(hostname+link['href'])   \n",
        "        else:\n",
        "            Link['externals'].append(link['href'])\n",
        "\n",
        "    for script in soup.find_all('script', src=True):\n",
        "        dots = [x.start(0) for x in re.finditer('\\.', script['src'])]\n",
        "        if hostname in script['src'] or domain in script['src'] or len(dots) == 1 or not script['src'].startswith('http'):\n",
        "            if not script['src'].startswith('http'):\n",
        "                if not script['src'].startswith('/'):\n",
        "                    Link['internals'].append(hostname+'/'+script['src']) \n",
        "                elif script['src'] in Null_format:\n",
        "                    Link['null'].append(script['src'])  \n",
        "                else:\n",
        "                    Link['internals'].append(hostname+script['src'])   \n",
        "        else:\n",
        "            Link['externals'].append(link['href'])\n",
        "  \n",
        "  def links_in_tags(Link):\n",
        "    total = len(Link['internals']) +  len(Link['externals'])\n",
        "    internals = len(Link['internals'])\n",
        "    try:\n",
        "        percentile = internals / float(total) * 100\n",
        "    except:\n",
        "        return 0\n",
        "    return percentile\n",
        "  #print(links_in_tags(Link))\n",
        "  if links_in_tags(Link) < 17:\n",
        "    return -1\n",
        "  elif links_in_tags(Link) >= 17 and links_in_tags(Link) <= 81:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuyX7gou0lEF",
        "outputId": "949dfefb-ca39-4e9d-f76f-275e17fc9bdc"
      },
      "source": [
        "links_tag('https://www.xiaoji.com/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract) (1.5.1)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.23.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.10)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract) (3.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from requests-file>=1.4->tldextract) (1.15.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFbL5g40AHXG"
      },
      "source": [
        "###SFH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K3HAagUAJiq"
      },
      "source": [
        "def sfh(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  \n",
        "  Form = {'internals':[], 'externals':[], 'null':[]}\n",
        "\n",
        "  def findForm(Form, domain, hostname):\n",
        "    for form in soup.findAll('form', action=True):\n",
        "      dots = [x.start(0) for x in re.finditer('\\.', form['action'])]\n",
        "      if hostname in form['action'] or domain in form['action'] or len(dots) == 1 or not form['action'].startswith('http'):\n",
        "          if not form['action'].startswith('http'):\n",
        "              if not form['action'].startswith('/'):\n",
        "                  Form['internals'].append(hostname+'/'+form['action']) \n",
        "              elif form['action'] in Null_format or form['action'] == 'about:blank':\n",
        "                  Form['null'].append(form['action'])  \n",
        "              else:\n",
        "                  Form['internals'].append(hostname+form['action'])   \n",
        "      else:\n",
        "          Form['externals'].append(form['action'])\n",
        "  \n",
        "  def sf(hostname, Form):\n",
        "    if len(Form['null'])==0:\n",
        "        return 1\n",
        "    elif len(Form['null'])>0:\n",
        "        return 0\n",
        "    else:\n",
        "      return -1\n",
        "  \n",
        "  return(sf(hostname, Form))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tQjn-8NAZK-",
        "outputId": "e7e88e82-dc8f-45e6-87d5-32ee15d950ac"
      },
      "source": [
        "sfh('https://stackoverflow.com/questions/59020008/how-to-import-functions-of-a-jupyter-notebook-into-another-jupyter-notebook-in-g')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.23.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract) (3.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from requests-file>=1.4->tldextract) (1.15.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9T17m9VotY_"
      },
      "source": [
        "### Submitting to emial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRQcMGYr1V5L"
      },
      "source": [
        "def sub_email(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  \n",
        "  Form = {'internals':[], 'externals':[], 'null':[]}\n",
        "  def findForm(Form, domain, hostname):\n",
        "    for form in soup.findAll('form', action=True):\n",
        "      dots = [x.start(0) for x in re.finditer('\\.', form['action'])]\n",
        "      if hostname in form['action'] or domain in form['action'] or len(dots) == 1 or not form['action'].startswith('http'):\n",
        "          if not form['action'].startswith('http'):\n",
        "              if not form['action'].startswith('/'):\n",
        "                  Form['internals'].append(hostname+'/'+form['action']) \n",
        "              elif form['action'] in Null_format or form['action'] == 'about:blank':\n",
        "                  Form['null'].append(form['action'])  \n",
        "              else:\n",
        "                  Form['internals'].append(hostname+form['action'])   \n",
        "      else:\n",
        "          Form['externals'].append(form['action'])\n",
        "  \n",
        "  def submitting_to_email(Form):\n",
        "    # print(Form)\n",
        "    for form in (Form['internals'] + Form['externals']):\n",
        "        #print('inside for')\n",
        "        if \"mailto:\" in form or \"mail()\" in form:\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "    return 1\n",
        "  \n",
        "  return(submitting_to_email(Form))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvn4GtcK2Ygi"
      },
      "source": [
        "# print('ans is ',  sub_email('https://www.xiaoji.com/'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmdyOA2u9J9U"
      },
      "source": [
        "### On Mouse-over"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSjdgTX49VKn"
      },
      "source": [
        "def mouse_over(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  \n",
        "  def onmouseover(content):\n",
        "    if 'onmouseover=\"window.status=' in str(content).lower().replace(\" \",\"\"):\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "  return(onmouseover(content.decode('latin-1')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIVx7oKD9i4R",
        "outputId": "eec02752-37de-4101-f6e6-da87f73be87b"
      },
      "source": [
        "mouse_over('https://www.xiaoji.com/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.23.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract) (3.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from requests-file>=1.4->tldextract) (1.15.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsn8SE8Z7a8r"
      },
      "source": [
        "### Right Click"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsfv_qCv7d_v"
      },
      "source": [
        "def right_click(url):\n",
        "  from urllib.parse import urlparse,urlencode,urlsplit, urlunsplit\n",
        "  from bs4 import BeautifulSoup\n",
        "  import requests\n",
        "  import re\n",
        "  !pip install tldextract\n",
        "  import tldextract\n",
        "  Null_format = [\"\", \"#\", \"#nothing\", \"#doesnotexist\", \"#null\", \"#void\", \"#whatever\",\n",
        "               \"#content\", \"javascript::void(0)\", \"javascript::void(0);\", \"javascript::;\", \"javascript\"]\n",
        "  \n",
        "  def is_URL_accessible(url):\n",
        "    page = None\n",
        "    try:\n",
        "        page = requests.get(url, timeout=5)   \n",
        "    except:\n",
        "        parsed = urlparse(url)\n",
        "        url = parsed.scheme+'://'+parsed.netloc\n",
        "        if not parsed.netloc.startswith('www'):\n",
        "            url = parsed.scheme+'://www.'+parsed.netloc\n",
        "            try:\n",
        "                page = requests.get(url, timeout=5)\n",
        "            except:\n",
        "                page = None\n",
        "                pass\n",
        "    if page and page.status_code == 200 and page.content not in [\"b''\", \"b' '\"]:\n",
        "        return True, url, page\n",
        "    else:\n",
        "        return False, None, None\n",
        "    \n",
        "  state, iurl, page = is_URL_accessible(url)\n",
        "\n",
        "  def get_domain(url):\n",
        "      o = urlsplit(url)\n",
        "      return o.hostname, tldextract.extract(url).domain, o.path\n",
        "    \n",
        "  if state:\n",
        "        content = page.content\n",
        "        hostname, domain, path = get_domain(url)\n",
        "  def right_clic(content):\n",
        "    if re.findall(r\"event.button ?== ?2\", content.decode('latin-1')):\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "  \n",
        "  return right_clic(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOK7rSVZ7vhG",
        "outputId": "a1ffd1af-e5c5-43e9-b8d1-f856c392e393"
      },
      "source": [
        "right_click('http://walletconnectbits.com/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract) (3.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.23.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract) (1.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from requests-file>=1.4->tldextract) (1.15.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPr_9zxxPK8m"
      },
      "source": [
        "### Domain Age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txf3wxJLPLX-"
      },
      "source": [
        "def domain_age(domain):\n",
        "    import json\n",
        "    import requests\n",
        "    url = domain.split(\"//\")[-1].split(\"/\")[0].split('?')[0]\n",
        "    show = \"https://input.payapi.io/v1/api/fraud/domain/age/\" + url\n",
        "    r = requests.get(show)\n",
        "\n",
        "    if r.status_code == 200:\n",
        "        data = r.text\n",
        "        jsonToPython = json.loads(data)\n",
        "        result = jsonToPython['result']\n",
        "        if result/30 >= 6:\n",
        "            return -1\n",
        "        else:\n",
        "            return 1\n",
        "    else:       \n",
        "        return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOHwDfCvPY9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e965d1b-1001-43fd-a970-9383619be6d0"
      },
      "source": [
        "domain_age('http://walletconnectbits.com/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG7HHBmbloIs"
      },
      "source": [
        "### DNS Record"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_K7UyfSlh5k"
      },
      "source": [
        "def dns_record(domain):\n",
        "    !pip install dnspython\n",
        "    import dns.resolver\n",
        "    try:\n",
        "        nameservers = dns.resolver.resolve(domain,'NS')\n",
        "        if len(nameservers) > 0:\n",
        "            #print('len is', nameservers)\n",
        "            return -1\n",
        "        else:\n",
        "            return 1\n",
        "    except:\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu5ITC0wmPTl"
      },
      "source": [
        "# dns_record('http://postdebanks.com/DIE/POST/diepost/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS_bgn_U65Rz"
      },
      "source": [
        "###Website Traffic "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMhbbEtg3wkR"
      },
      "source": [
        "def web_traffic(url):\n",
        "  from urllib.parse import quote\n",
        "  from urllib.request import urlopen\n",
        "  from bs4 import BeautifulSoup\n",
        "  try:\n",
        "    #Filling the whitespaces in the URL if any\n",
        "    url = quote(url)\n",
        "    rank = BeautifulSoup(urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\n",
        "        \"REACH\")['RANK']\n",
        "    rank = int(rank)\n",
        "  except TypeError:\n",
        "        return 1\n",
        "  if rank <= 100000:\n",
        "    return -1\n",
        "  elif rank > 100000:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxqEciG03zj8"
      },
      "source": [
        "#web_traffic('http://postdebanks.com/DIE/POST/diepost/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCb96p6XDn3R"
      },
      "source": [
        "### Page Rank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FfrJ0ZJDq0A"
      },
      "source": [
        "def page_rank(domain):\n",
        "    import requests\n",
        "    key = '8o0gg0804g4k0gwkk4oocws04oc0sg88gg844o4k'\n",
        "    url = 'https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=' + str(domain)\n",
        "    try:\n",
        "        request = requests.get(url, headers={'API-OPR': key})\n",
        "        result = request.json()\n",
        "        result = result['response'][0]['page_rank_integer']\n",
        "        if result / 10 < 0.2 :\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "    except:\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jFn7h2TDtW6"
      },
      "source": [
        "#page_rank('http://postdebanks.com/DIE/POST/diepost/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHl_tpunEUv5"
      },
      "source": [
        "### Google Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZunJvIiF71Mj"
      },
      "source": [
        "def google_index(url):\n",
        "    from urllib.parse import urlencode\n",
        "    from bs4 import BeautifulSoup\n",
        "    import requests\n",
        "    #time.sleep(.6)\n",
        "    user_agent =  'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'\n",
        "    headers = {'User-Agent' : user_agent}\n",
        "    query = {'q': 'site:' + url}\n",
        "    google = \"https://www.google.com/search?\" + urlencode(query)\n",
        "    data = requests.get(google, headers=headers)\n",
        "    data.encoding = 'ISO-8859-1'\n",
        "    soup = BeautifulSoup(str(data.content), \"html.parser\")\n",
        "    try:\n",
        "      if 'Our systems have detected unusual traffic from your computer network.' in str(soup):\n",
        "        return -1\n",
        "      check = soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"a\")\n",
        "      #print(soup.prettify())\n",
        "      if check and check['href']:\n",
        "        return -1\n",
        "      else:\n",
        "        return 1\n",
        "        \n",
        "    except AttributeError:\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVJtujPgE1nR"
      },
      "source": [
        "#google_index('http://walletconnectbits.com/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDMarF2_W-zH"
      },
      "source": [
        "### Links pointing to page (No of Hyperlinks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVHhoiU5W-Ym"
      },
      "source": [
        "def urlsCount(url):\n",
        "  from bs4 import BeautifulSoup\n",
        "  from collections import Counter\n",
        "  import requests\n",
        "  soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
        "  foundUrls = Counter([link[\"href\"] for link in soup.find_all(\"a\", href=lambda href: href and not href.startswith(\"#\"))])\n",
        "  count = len(foundUrls)\n",
        "  if(count == 0):\n",
        "    return 1\n",
        "  elif(count > 0 and count <= 2):\n",
        "    return 0\n",
        "  else:\n",
        "    return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxoP_8gTXr9G"
      },
      "source": [
        "#urlsCount('https://www.hokabutypolska.pl/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhbqtHmnYbBo"
      },
      "source": [
        "### statistical report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KehpaMN_YaT-"
      },
      "source": [
        "def statistical_report(url, domain):\n",
        "    import re\n",
        "    import socket\n",
        "    url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
        "    try:\n",
        "        ip_address=socket.gethostbyname(domain)\n",
        "        ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
        "                           '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
        "                           '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
        "                           '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
        "                           '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
        "                           '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)\n",
        "        if url_match or ip_match:\n",
        "            return 1\n",
        "        else:\n",
        "            return -1\n",
        "    except:\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ANKeJXpYcom"
      },
      "source": [
        "# statistical_report('http://postdebanks.com/DIE/POST/diepost/', 'postdebanks.com')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7zExvXQL5TU"
      },
      "source": [
        "### Input features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjkmmJ-DL9eQ"
      },
      "source": [
        "def getInput(url):\n",
        "  from urllib.parse import urlparse\n",
        "  domain = urlparse(url).netloc\n",
        "  input = []\n",
        "  print(domain)\n",
        "  input.append(having_ip_address(url))\n",
        "  input.append(URL_Length(url))\n",
        "  input.append(haveAtSign(url))\n",
        "  input.append(prefixSuffix(url))\n",
        "  input.append(sub_domain_count(url))\n",
        "  input.append(sslVerify(url))\n",
        "  input.append(port(domain))\n",
        "  input.append(request_url(url))\n",
        "  input.append(url_anchor(url))\n",
        "  input.append(links_tag(url))\n",
        "  input.append(sfh(url))\n",
        "  input.append(sub_email(url))\n",
        "  input.append(mouse_over(url))\n",
        "  input.append(right_click(url))\n",
        "  input.append(domain_age(url))\n",
        "  input.append(dns_record(url))\n",
        "  input.append(web_traffic(url))\n",
        "  input.append(page_rank(domain))\n",
        "  input.append(google_index(url))\n",
        "  input.append(urlsCount(url))\n",
        "  input.append(statistical_report(url, domain))\n",
        "  return (input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-xdaTIIQYY6"
      },
      "source": [
        "input = getInput('https://post-u8719-sufficient-159.sites.qsandbox.com/swisse/Home/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2zza5l7aL-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019b934b-a1bf-4bda-a082-6cc7c63ea02d"
      },
      "source": [
        "input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1, 0, -1, 1, 1, 0, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 0, 1, 1, 1, -1]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOo8d7PXaqs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff7c2cf-bfa9-43ce-db99-8f249ef427f1"
      },
      "source": [
        "[input]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-1, 0, -1, 1, 1, 0, 1, -1, 1, -1, 1, -1, -1, -1, -1, 1, 0, 1, 1, 1, -1]]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5ZTB0JmbhqR"
      },
      "source": [
        "import pickle\n",
        "filename = '/content/finalized_model.sav'\n",
        "loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjwL-IeS6GHe"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoM3UmnmaogS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2995010-7806-4fae-d0d1-0112d2975839"
      },
      "source": [
        "result = loaded_model.predict([input])\n",
        "if result == -1:\n",
        "  print(\"A legitimate website\")\n",
        "else:\n",
        "  print(\"A Phishing website!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Phishing website!!\n"
          ]
        }
      ]
    }
  ]
}